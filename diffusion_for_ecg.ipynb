{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2108c1ad-9735-4a8f-ac9c-694880e17ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Breakdown\n",
    "# PyTorch Libraries:\n",
    "\n",
    "# torch - Core PyTorch library for tensor operations and neural network training\n",
    "# torch.nn - Neural network modules and building blocks\n",
    "\n",
    "# Numerical & Utilities:\n",
    "\n",
    "# numpy - Numerical computing library for array operations\n",
    "# tqdm - Progress bar utility for tracking iterations during training/inference loops\n",
    "\n",
    "# Dataset:\n",
    "\n",
    "# keras.datasets.mnist.load_data - Loads the MNIST dataset (handwritten digits 0-9, 28x28 grayscale images)\n",
    "\n",
    "# Model Architecture:\n",
    "\n",
    "# unet.UNet - Imports a U-Net model (a custom implementation from a local file)\n",
    "\n",
    "# U-Net is a convolutional neural network architecture commonly used in image segmentation and diffusion models\n",
    "# Features encoder-decoder structure with skip connections\n",
    "# Well-suited for image-to-image tasks like denoising\n",
    "\n",
    "\n",
    "\n",
    "# Visualization:\n",
    "\n",
    "# matplotlib.pyplot - Plotting library for visualizing images, training curves, or generated samples\n",
    "\n",
    "# Typical Use Case\n",
    "# This setup is commonly used for:\n",
    "\n",
    "# Training a diffusion model to generate MNIST digits\n",
    "# Denoising images using a U-Net architecture\n",
    "# Experimenting with generative models on simple image data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras.datasets.mnist import load_data\n",
    "\n",
    "from unet import UNet\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d331f7c-208a-40d8-a393-9a8455d35307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "# trainX: training images (60,000 samples, 28x28 pixels)\n",
    "# trainy: training labels (digits 0-9)\n",
    "# testX: test images (10,000 samples, 28x28 pixels)\n",
    "# testy: test labels (digits 0-9)\n",
    "(trainX, trainy), (testX, testy) = load_data()\n",
    "\n",
    "# Normalize pixel values from [0, 255] to [0, 1] range\n",
    "# Convert to float32 for efficient computation with neural networks\n",
    "# Division by 255 scales the pixel intensities to the range [0, 1]\n",
    "trainX = np.float32(trainX) / 255.\n",
    "testX = np.float32(testX) / 255.\n",
    "\n",
    "def sample_batch(batch_size, device):\n",
    "    \"\"\"\n",
    "    Creates a random batch of training images for model training.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Number of images to sample from the training set\n",
    "        device: PyTorch device (cpu or cuda) where tensors will be stored\n",
    "    \n",
    "    Returns:\n",
    "        Batch of images as a tensor with shape (batch_size, 1, 32, 32)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate random indices to select images from the training set\n",
    "    # torch.randperm creates a random permutation of numbers from 0 to trainX.shape[0]-1\n",
    "    # [:batch_size] takes only the first 'batch_size' random indices\n",
    "    indices = torch.randperm(trainX.shape[0])[:batch_size]\n",
    "    \n",
    "    # Extract the selected images and convert to PyTorch tensor\n",
    "    # .unsqueeze(1) adds a channel dimension: (batch_size, 28, 28) -> (batch_size, 1, 28, 28)\n",
    "    # This is needed because PyTorch expects images in format: (batch, channels, height, width)\n",
    "    # .to(device) moves the tensor to the specified device (GPU or CPU)\n",
    "    data = torch.from_numpy(trainX[indices]).unsqueeze(1).to(device)\n",
    "    \n",
    "    # Resize images from 28x28 to 32x32 pixels using bilinear interpolation\n",
    "    # This is common in diffusion models as 32x32 works better with certain architectures\n",
    "    # interpolate() performs the upsampling operation\n",
    "    return torch.nn.functional.interpolate(data, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe74a9e6-875c-46de-808e-69a6d4807ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModel():\n",
    "    \"\"\"\n",
    "    Implementation of Denoising Diffusion Probabilistic Models (DDPM).\n",
    "    This class handles both the forward diffusion process (adding noise) \n",
    "    and reverse process (denoising/generation).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, T : int, model : nn.Module, device : str):\n",
    "        \"\"\"\n",
    "        Initialize the diffusion model.\n",
    "        \n",
    "        Args:\n",
    "            T: Number of diffusion timesteps (e.g., 1000)\n",
    "            model: Neural network (U-Net) that predicts noise at each timestep\n",
    "            device: 'cuda' for GPU or 'cpu' for CPU computation\n",
    "        \"\"\"\n",
    "        \n",
    "        # Total number of diffusion steps\n",
    "        self.T = T\n",
    "        \n",
    "        # The neural network (U-Net) that learns to predict noise\n",
    "        # This is the core model that will be trained\n",
    "        self.function_approximator = model.to(device)\n",
    "        self.device = device\n",
    "        \n",
    "        # Beta schedule: controls how much noise is added at each timestep\n",
    "        # Starts small (1e-4) and increases to 0.02 over T steps\n",
    "        # Small values at start = gradual noise addition\n",
    "        self.beta = torch.linspace(1e-4, 0.02, T).to(device)\n",
    "        \n",
    "        # Alpha = 1 - beta\n",
    "        # Represents how much of the original signal is retained\n",
    "        self.alpha = 1. - self.beta\n",
    "        \n",
    "        # Alpha_bar: cumulative product of alphas\n",
    "        # Represents total signal retention from step 0 to step t\n",
    "        # This allows us to jump directly to any timestep without iterating\n",
    "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
    "    \n",
    "    def training(self, batch_size, optimizer):\n",
    "        \"\"\"\n",
    "        Training step following Algorithm 1 from DDPM paper.\n",
    "        Trains the model to predict the noise that was added to images.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: Number of images to train on in this step\n",
    "            optimizer: PyTorch optimizer (e.g., Adam) for updating model weights\n",
    "            \n",
    "        Returns:\n",
    "            loss: The mean squared error between predicted and actual noise\n",
    "        \"\"\"\n",
    "        \n",
    "        # Step 1: Get a batch of clean images (x0) from the dataset\n",
    "        x0 = sample_batch(batch_size, self.device)\n",
    "        \n",
    "        # Step 2: Randomly sample a timestep t for each image in the batch\n",
    "        # Each image will have noise added corresponding to a random timestep\n",
    "        # Range: 1 to T (inclusive)\n",
    "        t = torch.randint(1, self.T + 1, (batch_size,), device=self.device, dtype=torch.long)\n",
    "        \n",
    "        # Step 3: Sample random noise (epsilon) with same shape as x0\n",
    "        # This is the \"true\" noise that will be added to the images\n",
    "        eps = torch.randn_like(x0)\n",
    "        \n",
    "        # Step 4: Get alpha_bar for the sampled timesteps\n",
    "        # unsqueeze operations reshape it to broadcast correctly with image dimensions\n",
    "        # Shape: (batch_size, 1, 1, 1) to match (batch_size, channels, height, width)\n",
    "        alpha_bar_t = self.alpha_bar[t-1].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        \n",
    "        # Step 5: Create noisy images using the closed-form forward diffusion equation\n",
    "        # x_t = sqrt(alpha_bar_t) * x0 + sqrt(1 - alpha_bar_t) * epsilon\n",
    "        # This adds the appropriate amount of noise for timestep t\n",
    "        # The model receives both the noisy image and the timestep t\n",
    "        eps_predicted = self.function_approximator(torch.sqrt(\n",
    "            alpha_bar_t) * x0 + torch.sqrt(1 - alpha_bar_t) * eps, t-1)\n",
    "        \n",
    "        # Step 6: Calculate loss - how well did the model predict the noise?\n",
    "        # MSE loss between actual noise (eps) and predicted noise (eps_predicted)\n",
    "        loss = nn.functional.mse_loss(eps, eps_predicted)\n",
    "        \n",
    "        # Step 7: Backpropagation - standard PyTorch training steps\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        loss.backward()         # Compute gradients\n",
    "        optimizer.step()        # Update model weights\n",
    "        \n",
    "        return loss.item()  # Return loss value for monitoring\n",
    "    \n",
    "    @torch.no_grad()  # Disable gradient calculation for efficiency during sampling\n",
    "    def sampling(self, n_samples=1, image_channels=1, img_size=(32, 32), use_tqdm=True):\n",
    "        \"\"\"\n",
    "        Generate new images by iteratively denoising random noise.\n",
    "        Implements Algorithm 2 from DDPM paper (reverse diffusion process).\n",
    "        \n",
    "        Args:\n",
    "            n_samples: Number of images to generate\n",
    "            image_channels: Number of color channels (1 for grayscale, 3 for RGB)\n",
    "            img_size: Tuple of (height, width) for generated images\n",
    "            use_tqdm: Whether to show a progress bar\n",
    "            \n",
    "        Returns:\n",
    "            Generated images as tensor of shape (n_samples, channels, height, width)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Step 1: Start from pure random noise (x_T)\n",
    "        # This is the completely noisy starting point\n",
    "        x = torch.randn((n_samples, image_channels, img_size[0], img_size[1]), \n",
    "                         device=self.device)\n",
    "        \n",
    "        # Optional progress bar for visualization\n",
    "        progress_bar = tqdm if use_tqdm else lambda x : x\n",
    "        \n",
    "        # Step 2: Iteratively denoise from timestep T down to 1\n",
    "        # Going backwards: T, T-1, T-2, ..., 2, 1\n",
    "        for t in progress_bar(range(self.T, 0, -1)):\n",
    "            \n",
    "            # Sample random noise z for stochastic sampling\n",
    "            # At the last step (t=1), use zeros instead (deterministic final step)\n",
    "            z = torch.randn_like(x) if t > 1 else torch.zeros_like(x)\n",
    "            \n",
    "            # Create a tensor of the current timestep for all samples in batch\n",
    "            # Shape: (n_samples,) all filled with current timestep value\n",
    "            t = torch.ones(n_samples, dtype=torch.long, device=self.device) * t \n",
    "            \n",
    "            # Get diffusion parameters for current timestep t\n",
    "            # Reshape to (n_samples, 1, 1, 1) for broadcasting\n",
    "            beta_t = self.beta[t-1].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "            alpha_t = self.alpha[t-1].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "            alpha_bar_t = self.alpha_bar[t-1].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "            \n",
    "            # Step 3: Calculate the mean of the denoising distribution\n",
    "            # This is the reverse diffusion equation from the DDPM paper\n",
    "            # Uses the model's noise prediction to estimate the less noisy image\n",
    "            mean = 1 / torch.sqrt(alpha_t) * (x - ((1 - alpha_t) / torch.sqrt(\n",
    "                1 - alpha_bar_t)) * self.function_approximator(x, t-1))\n",
    "            \n",
    "            # Step 4: Calculate the standard deviation for adding stochasticity\n",
    "            sigma = torch.sqrt(beta_t)\n",
    "            \n",
    "            # Step 5: Sample from the denoising distribution\n",
    "            # x_{t-1} = mean + sigma * z\n",
    "            # This gives us a slightly less noisy image\n",
    "            x = mean + sigma * z\n",
    "    \n",
    "        # After all iterations, x should be a clean generated image\n",
    "        return x\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a461fd6-830e-408c-8dfb-4e7cdbaa8ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the device for computation\n",
    "# 'cuda' = use GPU for faster training (requires NVIDIA GPU with CUDA support)\n",
    "# Alternative: 'cpu' for CPU computation (slower but works on any machine)\n",
    "device = 'cuda'\n",
    "\n",
    "# Batch size: number of images processed simultaneously in each training step\n",
    "# Larger batch sizes:\n",
    "#   - Faster training (better GPU utilization)\n",
    "#   - More memory required\n",
    "#   - More stable gradients\n",
    "# 64 is a common choice balancing speed and memory usage\n",
    "batch_size = 64\n",
    "\n",
    "# Initialize the U-Net neural network\n",
    "# This is the core architecture that will learn to predict noise\n",
    "# U-Net features:\n",
    "#   - Encoder-decoder structure with skip connections\n",
    "#   - Good at preserving spatial information\n",
    "#   - Widely used in image generation and segmentation tasks\n",
    "model = UNet()\n",
    "\n",
    "# Initialize the Adam optimizer for training the model\n",
    "# Adam is an adaptive learning rate optimizer (popular for deep learning)\n",
    "# Parameters:\n",
    "#   - model.parameters(): all trainable weights in the U-Net\n",
    "#   - lr=2e-5: learning rate (0.00002)\n",
    "#     * Small learning rate for stable training\n",
    "#     * Prevents overshooting optimal weights\n",
    "#     * Common for diffusion models\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Create the diffusion model instance\n",
    "# This wraps the U-Net and handles the diffusion process\n",
    "# Parameters:\n",
    "#   - 1000: Number of diffusion timesteps (T)\n",
    "#     * More steps = smoother noise addition/removal\n",
    "#     * Standard choice in DDPM paper\n",
    "#     * Trade-off between quality and computation time\n",
    "#   - model: The U-Net that predicts noise\n",
    "#   - device: Where computations happen (GPU in this case)\n",
    "diffusion_model = DiffusionModel(1000, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a04313e-cfb9-46f1-ab37-e90cb1719cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40000/40000 [2:42:05<00:00,  4.11it/s]    \n"
     ]
    }
   ],
   "source": [
    "# List to store training loss values for monitoring progress\n",
    "# Each epoch's loss will be appended here for visualization\n",
    "training_loss = []\n",
    "\n",
    "# Training loop: iterate for 40,000 epochs (training iterations)\n",
    "# tqdm creates a progress bar showing training progress\n",
    "# 40_000 uses underscore for readability (same as 40000)\n",
    "for epoch in tqdm(range(40_000)):\n",
    "    \n",
    "    # Perform one training step\n",
    "    # - Samples a batch of images\n",
    "    # - Adds noise at random timesteps\n",
    "    # - Trains model to predict the noise\n",
    "    # - Returns the loss value (how well model predicted noise)\n",
    "    loss = diffusion_model.training(batch_size, optimizer)\n",
    "    \n",
    "    # Store the loss for this epoch\n",
    "    # Allows us to track if model is improving over time\n",
    "    training_loss.append(loss)\n",
    "    \n",
    "    # Every 100 epochs, save loss plots to visualize training progress\n",
    "    if epoch % 100 == 0:\n",
    "        \n",
    "        # Plot 1: Complete training history\n",
    "        # Shows loss from epoch 0 to current epoch\n",
    "        # Useful to see overall trend and convergence\n",
    "        plt.plot(training_loss)\n",
    "        plt.savefig('training_loss.png')\n",
    "        plt.close()  # Close figure to free memory\n",
    "        \n",
    "        # Plot 2: Recent training history (last 1000 epochs)\n",
    "        # Zoomed-in view of recent performance\n",
    "        # Useful to see if loss is still decreasing or has plateaued\n",
    "        plt.plot(training_loss[-1000:])  # [-1000:] takes last 1000 elements\n",
    "        plt.savefig('training_loss_cropped.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Every 5000 epochs, generate sample images and save model checkpoint\n",
    "    if epoch % 5000 == 0:\n",
    "        \n",
    "        # Generate 81 images (9x9 grid)\n",
    "        # This helps visually assess model quality during training\n",
    "        nb_images = 81\n",
    "        \n",
    "        # Use the model to generate new images from random noise\n",
    "        # use_tqdm=False: don't show progress bar for sampling (cleaner output)\n",
    "        samples = diffusion_model.sampling(n_samples=nb_images, use_tqdm=False)\n",
    "        \n",
    "        # Create a large figure to display all generated images\n",
    "        # figsize=(17, 17): 17 inches x 17 inches for clear visualization\n",
    "        plt.figure(figsize=(17, 17))\n",
    "        \n",
    "        # Loop through all generated images and arrange in 9x9 grid\n",
    "        for i in range(nb_images):\n",
    "            plt.subplot(9, 9, 1 + i)  # Create subplot in 9x9 grid, position 1+i\n",
    "            plt.axis('off')  # Hide axis labels and ticks for cleaner image\n",
    "            \n",
    "            # Display the image:\n",
    "            # - .squeeze(0): Remove channel dimension (1, 32, 32) -> (32, 32)\n",
    "            # - .clip(0, 1): Clamp pixel values to valid range [0, 1]\n",
    "            # - .data: Access underlying tensor data\n",
    "            # - .cpu(): Move tensor from GPU to CPU (required for numpy)\n",
    "            # - .numpy(): Convert PyTorch tensor to numpy array (for matplotlib)\n",
    "            # - cmap='gray': Display as grayscale image\n",
    "            plt.imshow(samples[i].squeeze(0).clip(0, 1).data.cpu().numpy(), cmap='gray')\n",
    "        \n",
    "        # Save the grid of generated images with epoch number in filename\n",
    "        # Allows tracking of generation quality improvement over training\n",
    "        plt.savefig(f'samples_epoch_{epoch}.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Save model checkpoint\n",
    "        # model.cpu(): Move model to CPU before saving (reduces file size)\n",
    "        # Checkpoint naming includes epoch for version tracking\n",
    "        torch.save(model.cpu(), f'model_paper2_epoch_{epoch}')\n",
    "        \n",
    "        # Move model back to GPU for continued training\n",
    "        model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2fc9a9-59db-4588-bc92-a4ed3f13c75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of images to generate\n",
    "# 81 images = 9 rows Ã— 9 columns (perfect square for grid display)\n",
    "nb_images = 81\n",
    "\n",
    "# Generate new images using the trained diffusion model\n",
    "# This is the inference/sampling process:\n",
    "# - Starts from pure random noise\n",
    "# - Iteratively denoises through all 1000 timesteps\n",
    "# - Returns clean generated images\n",
    "# use_tqdm=False: Disables progress bar during sampling for cleaner output\n",
    "samples = diffusion_model.sampling(n_samples=nb_images, use_tqdm=False)\n",
    "\n",
    "# Create a large matplotlib figure to display all images\n",
    "# figsize=(17, 17): Creates a 17Ã—17 inch figure\n",
    "# Large size ensures each generated image is clearly visible\n",
    "plt.figure(figsize=(17, 17))\n",
    "\n",
    "# Loop through all generated images and arrange them in a grid\n",
    "for i in range(nb_images):\n",
    "    \n",
    "    # Create a subplot in a 9Ã—9 grid layout\n",
    "    # plt.subplot(rows, cols, position)\n",
    "    # - 9, 9: Grid dimensions (9 rows, 9 columns)\n",
    "    # - 1 + i: Position in grid (1-indexed, so starts at 1 not 0)\n",
    "    plt.subplot(9, 9, 1 + i)\n",
    "    \n",
    "    # Remove axis labels, ticks, and frame for cleaner image display\n",
    "    # Makes the grid look like a collection of images without distractions\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Display the generated image with proper preprocessing:\n",
    "    # samples[i]: Get the i-th generated image (shape: [1, 32, 32])\n",
    "    # .squeeze(0): Remove channel dimension [1, 32, 32] â†’ [32, 32]\n",
    "    #              (matplotlib expects 2D array for grayscale)\n",
    "    # .clip(0, 1): Clamp pixel values to valid range [0.0, 1.0]\n",
    "    #              (neural networks sometimes output values outside this range)\n",
    "    # .data: Access the underlying tensor data\n",
    "    # .cpu(): Move tensor from GPU memory to CPU memory\n",
    "    #         (matplotlib requires CPU arrays, not GPU tensors)\n",
    "    # .numpy(): Convert PyTorch tensor to NumPy array\n",
    "    #           (matplotlib works with NumPy arrays)\n",
    "    # cmap='gray': Use grayscale colormap for displaying single-channel images\n",
    "    plt.imshow(samples[i].squeeze(0).clip(0, 1).data.cpu().numpy(), cmap='gray')\n",
    "\n",
    "# Save the complete 9Ã—9 grid as a single image file\n",
    "# f-string allows inserting the current epoch number in filename\n",
    "# Example: 'samples_epoch_40000.png' if epoch is 40000\n",
    "# This creates a visual record of generation quality at this training stage\n",
    "plt.savefig(f'samples_epoch_{epoch}.png')\n",
    "\n",
    "# Close the figure to free up memory\n",
    "# Important in loops to prevent memory buildup from multiple figures\n",
    "plt.close()\n",
    "\n",
    "# Save the trained model as a checkpoint file\n",
    "# model.cpu(): Move model from GPU to CPU before saving\n",
    "#              - Reduces file size\n",
    "#              - Makes checkpoint compatible with systems without GPU\n",
    "# torch.save(): PyTorch's function to serialize and save model\n",
    "# f'model_paper2_epoch_{epoch}': Filename includes epoch for version tracking\n",
    "# Note: This saves the entire model object (architecture + weights)\n",
    "torch.save(model.cpu(), f'model_paper2_epoch_{epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35036368-d469-4b32-9ee4-e7fc850d3bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How It Works (Big Picture)\n",
    "# Forward Process (Training)\n",
    "\n",
    "# Take clean MNIST image\n",
    "# Add noise according to random timestep t\n",
    "# Train U-Net to predict that noise\n",
    "# Repeat 40,000 times\n",
    "\n",
    "# Reverse Process (Generation)\n",
    "\n",
    "# Start with pure random noise\n",
    "# Use trained U-Net to predict noise\n",
    "# Remove predicted noise\n",
    "# Repeat 1000 times\n",
    "# Result: Clean generated digit image\n",
    "\n",
    "\n",
    "# Key Concepts\n",
    "# Concept DescriptionDiffusion \n",
    "# Steps (T=1000)Number of gradual noise addition/removal steps\n",
    "# U-Net Neural network that learns to predict noise\n",
    "# Beta ScheduleControls how much noise is added at each step\n",
    "# Training Teach model to recognize noise patterns\n",
    "# Sampling Generate new images by denoising random noise\n",
    "# Checkpoints Save model at intervals for backup/evaluation\n",
    "\n",
    "# Expected Results\n",
    "\n",
    "# Training Loss: Should decrease over time (model improving)\n",
    "# Generated Images: Should look like handwritten digits (0-9)\n",
    "# Training Time: Several hours on GPU for 40,000 epochs\n",
    "# Output Files: Loss plots, sample grids, model checkpoints\n",
    "\n",
    "# This is a complete implementation of the DDPM paper for generating MNIST-style digit images! ðŸŽ¨ðŸ”¢"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
